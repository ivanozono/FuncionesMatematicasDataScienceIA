{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "6eb3c9e7-c17a-5f20-bbbc-2389b74c7a36",
        "openai_ephemeral_user_id": "3af8edfd-ae3a-58ef-80b4-27e8917550be",
        "openai_subdivision1_iso_code": "MX-BCN"
      }
    },
    "noteable": {
      "last_transaction_id": "83aee7f3-293c-428c-aa8d-3aa792a06e03"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "dfb0521a-356b-409e-96c9-363f1da6efb6",
      "cell_type": "markdown",
      "source": "# Mathematical Functions for Data Science and Artificial Intelligence\n\nIn this notebook, we will explore various mathematical functions that are crucial in the field of Data Science and Artificial Intelligence. We will cover a wide range of topics, from basic algebraic functions to more complex concepts like perceptrons in Machine Learning. We will also work with a dataset and apply these concepts in practice.\n\n## Outline\n\n1. Algebraic Functions\n2. Understanding Functions: f(x)\n3. Types of Variables\n4. Domain and Range of a Function\n5. Reading Mathematics: General Symbols\n6. Sets in Mathematics\n7. Polynomial Algebraic Functions\n8. Transcendental Functions\n9. Piecewise Functions\n10. Composite Functions\n11. Manipulation of Mathematical Functions\n12. Characteristics of Mathematical Functions\n13. Perceptron - A Type of Artificial Neuron in Machine Learning\n14. Activation Functions\n15. Simple Linear Regression\n16. Calculating Errors in Machine Learning and Linear Regression\n17. Data Analysis\n\nLet's get started!",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "5aecd80a-9461-463f-ace9-6dcc4f7f230b",
      "cell_type": "markdown",
      "source": "## 1. Algebraic Functions\n\nAlgebraic functions are a way to express a certain equation that depends on one or more variables. The variables in the equation do not have any restrictions on what they can be. This means that they can be any real number, any imaginary number, any variable, or any constant.\n\nAlgebraic functions are important in the world of mathematics because they are used to model and solve real-world problems. They are used in a variety of fields, including engineering, physics, and computer science.\n\nIn the context of Data Science and Machine Learning, algebraic functions are used to model relationships between variables and are often used in algorithms to learn from data.\n\nLet's start by importing the necessary libraries for our work.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "705d499c-a2bd-4acb-8f41-216260335385",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4d40f742-0844-4a92-9666-550c1a332b65"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:11:15.181892+00:00",
          "start_time": "2023-07-04T04:11:14.165994+00:00"
        },
        "datalink": {
          "ec915a9a-a8b6-41a1-b3d2-ad7791643820": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 14,
              "orig_num_rows": 5,
              "orig_size_bytes": 600,
              "truncated_num_cols": 14,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 600,
              "truncated_string_columns": []
            },
            "display_id": "ec915a9a-a8b6-41a1-b3d2-ad7791643820",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-04T04:11:15.022839",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_f35a8be93b4d486aa7d4301afd9997d2"
          }
        }
      },
      "execution_count": null,
      "source": "# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import load_boston\n\n# Loading the dataset\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['MEDV'] = boston.target\n\n# Displaying the first few rows of the dataset\ndf.head()",
      "outputs": []
    },
    {
      "id": "fd8b3809-d143-4e68-9b83-cea643aa69af",
      "cell_type": "markdown",
      "source": "We have loaded the Boston Housing dataset, which is a famous dataset used in Machine Learning. It contains information about different houses in Boston. There are 506 samples and 13 feature variables in this dataset. The objective is to predict the value of prices of the house using the given features.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "9bf9e7c3-ce41-42f6-bdb8-beaab9acebd5",
      "cell_type": "markdown",
      "source": "## 2. Understanding Functions: f(x)\n\nIn mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output. An example is the function that relates each real number x to its square xÂ². The output of a function f corresponding to an input x is denoted by f(x).\n\nIn the context of Machine Learning, functions are used to describe the relationship between inputs (features) and outputs (targets). For example, in linear regression, the relationship between the input x and the output y is described by the function y = mx + c, where m is the slope of the line and c is the y-intercept.\n\nLet's now create a simple linear function and plot it.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "c383222c-6f2b-45dd-8e1b-5589cb09d695",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "088fdb13-650b-4212-b48c-9bc2adc33107"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:12:12.178862+00:00",
          "start_time": "2023-07-04T04:12:11.540357+00:00"
        }
      },
      "execution_count": null,
      "source": "# Defining the function\ndef f(x):\n    return 2*x + 1\n\n# Generating x values\nx = np.linspace(-10, 10, 400)\n\n# Generating y values\ny = f(x)\n\n# Creating the plot\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Plot of the linear function f(x) = 2x + 1')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True)\nplt.show()",
      "outputs": []
    },
    {
      "id": "618bb193-aa41-4c4c-9e7a-599dc449f72a",
      "cell_type": "markdown",
      "source": "We have plotted the function f(x) = 2x + 1. As you can see, it's a straight line that crosses the y-axis at y = 1 (the y-intercept) and has a slope of 2.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "14c1bf2b-47e0-4e31-9be7-c38f4d09108d",
      "cell_type": "markdown",
      "source": "## 3. Types of Variables\n\nIn Data Science, we deal with different types of variables. Understanding the type of variables is important as it helps in choosing the right statistical analysis technique and in model building.\n\nThere are mainly two types of variables:\n\n1. **Quantitative variables**: These are numerical variables that can be measured. They can be further classified into two types:\n    - **Discrete variables**: These are countable variables. For example, the number of students in a class.\n    - **Continuous variables**: These are measurable variables. For example, height, weight, temperature etc.\n\n2. **Qualitative variables (or categorical variables)**: These are non-numerical variables that can be grouped into different categories. They can be further classified into two types:\n    - **Nominal variables**: These are categorical variables without any order or priority. For example, gender, marital status etc.\n    - **Ordinal variables**: These are categorical variables with a sense of order. For example, ratings, education level (high school, undergraduate, postgraduate etc.)\n\nLet's now explore the types of variables in our dataset.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "116b2184-a43d-40bf-91b2-bb8d16042209",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "901fccc0-2414-4129-8577-240f68a9ea43"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:12:59.975423+00:00",
          "start_time": "2023-07-04T04:12:59.786717+00:00"
        },
        "datalink": {
          "20795946-9ee6-4c67-ab92-66394c527edc": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 1,
              "orig_num_rows": 14,
              "orig_size_bytes": 224,
              "truncated_num_cols": 1,
              "truncated_num_rows": 14,
              "truncated_size_bytes": 224,
              "truncated_string_columns": []
            },
            "display_id": "20795946-9ee6-4c67-ab92-66394c527edc",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-07-04T04:12:59.819565",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_ae0baca28b9e4493a7a2c9aeb72ad869"
          }
        }
      },
      "execution_count": null,
      "source": "# Checking the data types of the variables\ndf.dtypes",
      "outputs": []
    },
    {
      "id": "af1d6593-da78-440f-a38f-7eff59d53677",
      "cell_type": "markdown",
      "source": "All the variables in our dataset are of the type 'float64', which means they are continuous quantitative variables.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "da66a731-aff9-46c8-92f4-7d42fdc16e17",
      "cell_type": "markdown",
      "source": "## 4. Domain and Range of a Function\n\nThe domain of a function is the complete set of possible values of the independent variable. In plain English, the definition means that the domain is the set of all possible x-values which will make the function \"work\", and will output real y-values.\n\nThe range of a function is the complete set of all possible resulting values of the dependent variable (y, usually), after we have substituted the domain.\n\nIn the context of Machine Learning, the domain of a function could be all possible input values (features), and the range of the function could be all possible output values (predictions).\n\nLet's now calculate the domain and range of our dataset.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "c43aec68-9746-4f9d-b9d8-eea544848101",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3f0f01bd-df8e-4ec9-b047-425353df876e"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:13:49.863189+00:00",
          "start_time": "2023-07-04T04:13:49.691518+00:00"
        }
      },
      "execution_count": null,
      "source": "# Calculating the domain and range of the dataset\ndomain = df.drop('MEDV', axis=1).apply(lambda x: (x.min(), x.max()), axis=0)\nrange_ = df['MEDV'].min(), df['MEDV'].max()\n\n# Printing the domain and range\nprint('Domain:')\nprint(domain)\nprint('\\nRange:')\nprint(range_)",
      "outputs": []
    },
    {
      "id": "4f77522c-7159-42a6-bb8b-317b1d9fa79a",
      "cell_type": "markdown",
      "source": "The domain of our dataset is the range of values that each feature (independent variable) can take, and the range is the range of values that the target variable ('MEDV') can take.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "a3194bce-2743-4289-abb4-d05d80eeac75",
      "cell_type": "markdown",
      "source": "## 5. Reading Mathematics: General Symbols\n\nIn mathematics, we use a lot of symbols to represent different operations, relations, constants, variables, etc. Understanding these symbols is crucial to understanding mathematical expressions and equations.\n\nHere are some of the most common mathematical symbols:\n\n- **+**: Plus sign, used for addition.\n- **-**: Minus sign, used for subtraction.\n- **Ã, *, Â·**: Multiplication signs.\n- **Ã·, /**: Division signs.\n- **=**: Equals sign, shows equality.\n- **â **: Not equals sign, shows inequality.\n- **<, >**: Less than and greater than signs.\n- **â¤, â¥**: Less than or equal to and greater than or equal to signs.\n- **( )**: Parentheses, used to group terms together.\n- **[ ]**: Brackets, also used to group terms together.\n- **{ }**: Braces, used to denote sets.\n- **â**: Sigma, used to represent summation.\n- **â**: Pi, used to represent product.\n- **â**: Square root.\n- **â**: Infinity.\n- **Ï**: Pi, a mathematical constant approximately equal to 3.14159.\n- **e**: Euler's number, a mathematical constant approximately equal to 2.71828.\n\nIn the context of Machine Learning, these symbols are used in mathematical equations to describe algorithms, calculate metrics, etc.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "e4806571-1ac1-462f-add9-ec627447dbd5",
      "cell_type": "markdown",
      "source": "## 6. Sets in Mathematics\n\nIn mathematics, a set is a collection of distinct objects, considered as an object in its own right. Sets are one of the most fundamental concepts in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived.\n\nIn Machine Learning, sets are used in various ways. For example, a dataset can be considered as a set of data points. When we divide the dataset into training set and test set, we are creating two distinct sets of data points.\n\nLet's now create a simple set of numbers.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "08f634b0-e3f4-4ae9-8e86-c4bbb9922c17",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1a3ad2d2-9c7d-4b03-918b-be4b29c22aaf"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:16:07.514631+00:00",
          "start_time": "2023-07-04T04:16:07.356056+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating a set of numbers\nnumbers = set([1, 2, 3, 4, 5])\n\n# Printing the set\nprint(numbers)",
      "outputs": []
    },
    {
      "id": "1c4d50e1-e5c2-445b-87f6-b0069156b37f",
      "cell_type": "markdown",
      "source": "We have created a set of numbers from 1 to 5. In Python, a set is an unordered collection of items. Every set element is unique (no duplicates) and must be immutable (cannot be changed).\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ac8e2bc9-922a-44b4-a164-b1b48c0fa5a0",
      "cell_type": "markdown",
      "source": "## 7. Algebraic Functions: Linear Functions\n\nAlgebraic functions are functions which can be expressed using arithmetic operations and whose values are either rational or a root of a rational number. An algebraic function is a type of function that is defined by a polynomial equation. The most common type of algebraic function is a polynomial function, with the term 'polynomial' meaning 'many terms'.\n\nA linear function is a polynomial function of degree 1. In a linear function, each term is either a constant or the product of a constant and a single variable. Linear functions are functions that produce a straight line graph. The standard form of a linear function is f(x) = mx + c, where m and c are constants.\n\nWe have already plotted a linear function in the previous sections. Let's now create a linear regression model, which is a machine learning algorithm based on linear functions.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "8d140708-2784-48e5-84aa-ebe4f79b0080",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8f133efa-8cf6-4f27-96c1-badf0f060ecf"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:17:12.337996+00:00",
          "start_time": "2023-07-04T04:17:12.170986+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Splitting the dataset into training set and test set\nX = df.drop('MEDV', axis=1)\ny = df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Creating a linear regression model\nmodel = LinearRegression()\n\n# Training the model\nmodel.fit(X_train, y_train)\n\n# Making predictions\ny_pred = model.predict(X_test)\n\n# Calculating the mean squared error\nmse = mean_squared_error(y_test, y_pred)\n\n# Printing the mean squared error\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "d65b32c3-2bb0-4d56-a542-15fc7cb535e7",
      "cell_type": "markdown",
      "source": "We have created a linear regression model and trained it on our dataset. The mean squared error of our model is approximately 24.29. This means that, on average, our model's predictions are about 24.29 units away from the actual values.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "69b7303f-1c5b-4769-9ae7-43b7b5292b8f",
      "cell_type": "markdown",
      "source": "## 8. Algebraic Functions: Polynomials\n\nA polynomial function is a type of algebraic function where the relationship between the input and the output is defined by a polynomial expression. Polynomial functions can be described by the equation:\n\nf(x) = a_n*x^n + a_(n-1)*x^(n-1) + ... + a_2*x^2 + a_1*x + a_0\n\nwhere:\n- n is a nonnegative integer\n- a_0, a_1, ..., a_n are constants\n- a_n â  0\n\nThe highest power of x in the polynomial is called the degree of the polynomial. The degree of the polynomial determines the most number of solutions that the function can have. For example, a linear function is a polynomial of degree 1, and it has one solution. A quadratic function is a polynomial of degree 2, and it has at most two solutions.\n\nIn the context of Machine Learning, polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial. Polynomial regression can be used to model relationships between variables that aren't linear.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "e4b10847-458e-4299-8409-4effe60845f5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "c59c3a4e-b080-4334-8744-44e716dab1a5"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:18:19.533182+00:00",
          "start_time": "2023-07-04T04:18:18.874602+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing necessary libraries\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Creating a PolynomialFeatures object\npoly = PolynomialFeatures(degree=2)\n\n# Transforming the features to higher degree features.\nX_train_poly = poly.fit_transform(X_train)\n\n# fit the transformed features to Linear Regression\npoly_model = LinearRegression()\npoly_model.fit(X_train_poly, y_train)\n\n# predicting on training data-set\ny_train_predicted = poly_model.predict(X_train_poly)\n\n# predicting on test data-set\ny_test_predict = poly_model.predict(poly.fit_transform(X_test))\n\n# evaluating the model on training dataset\nmse_train = mean_squared_error(y_train, y_train_predicted)\n\n# evaluating the model on test dataset\nmse_test = mean_squared_error(y_test, y_test_predict)\n\n# Printing the mean squared errors\nprint('Mean Squared Error (Training set):', mse_train)\nprint('Mean Squared Error (Test set):', mse_test)",
      "outputs": []
    },
    {
      "id": "53dfb702-1c5b-4ba4-84fd-6b93cef2bd7d",
      "cell_type": "markdown",
      "source": "We have created a polynomial regression model and trained it on our dataset. The mean squared error of our model on the training set is approximately 5.63, and on the test set is approximately 14.57. This means that, on average, our model's predictions are about 5.63 units away from the actual values on the training set, and about 14.57 units away on the test set.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "fd0af099-8293-42a0-bf82-689de37c4140",
      "cell_type": "markdown",
      "source": "## 9. Transcendental Functions\n\nTranscendental functions are functions that do not satisfy a polynomial equation, in contrast to algebraic functions. In other words, a transcendental function 'transcends' algebra in that it cannot be expressed in terms of a finite sequence of the algebraic operations of addition, multiplication, and root extraction.\n\nExamples of transcendental functions include exponential functions, logarithmic functions, and trigonometric functions.\n\nIn the context of Machine Learning, transcendental functions are often used in activation functions of neural networks. For example, the sigmoid function, which is a type of exponential function, is a commonly used activation function in neural networks.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "0b4d6f4f-309c-4e50-b58e-f1f21fde5bad",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a9022075-ab64-4bae-b289-6ff1b89273c7"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:19:25.254144+00:00",
          "start_time": "2023-07-04T04:19:24.785501+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Defining the sigmoid function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Generating a sequence of numbers from -10 to 10\nx = np.linspace(-10, 10, 100)\n\n# Applying the sigmoid function to the sequence of numbers\ny = sigmoid(x)\n\n# Plotting the sigmoid function\nplt.plot(x, y)\nplt.title('Sigmoid Function')\nplt.xlabel('x')\nplt.ylabel('sigmoid(x)')\nplt.grid(True)\nplt.show()",
      "outputs": []
    },
    {
      "id": "c980c83b-c1db-4235-94e5-65d376f62615",
      "cell_type": "markdown",
      "source": "We have plotted the sigmoid function, which is a type of transcendental function. The sigmoid function is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "1199809e-c509-48d5-befc-1d32321f4e8c",
      "cell_type": "markdown",
      "source": "## 10. Piecewise Functions\n\nA piecewise function is a function that is defined by several different formulas, or 'pieces', each of which applies to a different domain. Piecewise functions are used in many branches of mathematics, and they have important applications in physics, engineering, and computer science.\n\nIn the context of Machine Learning, piecewise functions are often used in activation functions of neural networks. For example, the ReLU (Rectified Linear Unit) function is a type of piecewise function that is used as an activation function in neural networks. The ReLU function is defined as:\n\nf(x) = max(0, x)\n\nThis means that the function returns x if x is greater than or equal to 0, and returns 0 otherwise.\n\nLet's now plot the ReLU function.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "d76b8416-48e8-4576-8f34-012e065294b9",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "# Defining the ReLU function\ndef relu(x):\n    return np.maximum(0, x)\n\n# Generating a sequence of numbers from -10 to 10\nx = np.linspace(-10, 10, 100)\n\n# Applying the ReLU function to the sequence of numbers\ny = relu(x)\n\n# Plotting the ReLU function\nplt.plot(x, y)\nplt.title('ReLU Function')\nplt.xlabel('x')\nplt.ylabel('ReLU(x)')\nplt.grid(True)\nplt.show()",
      "outputs": []
    },
    {
      "id": "b06e772c-c738-451b-8cd9-f0216a62a314",
      "cell_type": "markdown",
      "source": "We have plotted the ReLU function, which is a type of piecewise function. The ReLU function is commonly used as an activation function in neural networks because it introduces non-linearity into the model without requiring expensive computations.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "cbae8f95-15c0-4323-a0b9-3fdeb7045c83",
      "cell_type": "markdown",
      "source": "## 11. Composite Functions\n\nIn mathematics, a composite function is a function that is composed of two other functions. The composite function f(g(x)) is formed by applying the function g to x, and then applying the function f to the result.\n\nIn the context of Machine Learning, composite functions are used in various ways. For example, the process of training a neural network can be seen as finding the optimal composite function that maps the input data to the output data. Each layer in the neural network applies a function to the output of the previous layer, and these functions are composed together to form the final output of the network.\n\nLet's now create a simple example of a composite function.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "0021378b-23a7-478f-abac-ccb552dc75af",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ca2e4c47-0da8-4ec7-ab06-93d3355bf681"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:22:10.144388+00:00",
          "start_time": "2023-07-04T04:22:09.986052+00:00"
        }
      },
      "execution_count": null,
      "source": "# Defining two functions\ndef f(x):\n    return x ** 2\n\ndef g(x):\n    return x + 2\n\n# Defining the composite function\ndef h(x):\n    return f(g(x))\n\n# Applying the composite function to a number\nprint(h(3))",
      "outputs": []
    },
    {
      "id": "d28e6ab1-3e89-4f3c-a5e2-4a9e98d15dbe",
      "cell_type": "markdown",
      "source": "We have created a composite function h(x) = f(g(x)), where f(x) = x^2 and g(x) = x + 2. We applied the composite function to the number 3, and the result was 25. This is because g(3) = 3 + 2 = 5, and f(5) = 5^2 = 25.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "028b0b1e-00a7-4419-8ee0-cf4a29da9865",
      "cell_type": "markdown",
      "source": "## 12. Inverse Functions\n\nIn mathematics, an inverse function is a function that 'reverses' another function. If the function f applied to an input x gives a result of y, then applying its inverse function g to y gives the result x, and vice versa. i.e., f(x) = y if and only if g(y) = x.\n\nIn the context of Machine Learning, inverse functions are used in various ways. For example, the logarithm is the inverse function to exponentiation. Logarithms are used in various algorithms such as logistic regression, and they are also used in the calculation of information gain in decision trees.\n\nLet's now create a simple example of an inverse function.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "5efe2ba2-9507-41fe-8bf1-dc40d739802b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "37be4250-35c5-4333-870c-4b98f2e2d728"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:23:07.853900+00:00",
          "start_time": "2023-07-04T04:23:07.693886+00:00"
        }
      },
      "execution_count": null,
      "source": "# Defining two functions\ndef f(x):\n    return x ** 2\n\ndef g(y):\n    return np.sqrt(y)\n\n# Applying the function and its inverse\nx = 3\ny = f(x)\nx_inv = g(y)\n\n# Printing the results\nprint('x:', x)\nprint('f(x):', y)\nprint('g(f(x)):', x_inv)",
      "outputs": []
    },
    {
      "id": "693a8158-3b7e-4cc8-9e21-63f890612cc3",
      "cell_type": "markdown",
      "source": "We have created a function f(x) = x^2 and its inverse function g(y) = sqrt(y). We applied the function f to the number 3, and the result was 9. Then, we applied the inverse function g to the result, and we got back the original number 3. This demonstrates the concept of inverse functions.\n\nThis concludes our exploration of mathematical functions in the context of Machine Learning. We have covered a wide range of functions, from basic algebraic functions to more complex transcendental and piecewise functions. We have also seen how these functions are used in various Machine Learning algorithms and techniques. Understanding these functions and their properties is crucial for understanding and implementing Machine Learning algorithms.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "c7045ce7-c013-4267-911a-ec7d0903f7b8",
      "cell_type": "markdown",
      "source": "## 13. Manipulation of Mathematical Functions\n\nManipulation of mathematical functions involves operations such as addition, subtraction, multiplication, division, and composition on functions. These operations can result in new functions.\n\nIn the context of Machine Learning, manipulation of functions is a common task. For example, in the process of feature engineering, we often create new features by applying mathematical operations to existing features. This can help to capture complex relationships in the data and improve the performance of our models.\n\nLet's now create a simple example of function manipulation.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "79d00f4a-b487-4548-890c-dc84f623af7f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "2df0c267-9bb8-4eb3-a715-20aab4732172"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:30:16.237718+00:00",
          "start_time": "2023-07-04T04:30:16.079564+00:00"
        }
      },
      "execution_count": null,
      "source": "# Defining two functions\ndef f(x):\n    return x ** 2\n\ndef g(x):\n    return x + 2\n\n# Defining a new function that is the sum of f and g\ndef h(x):\n    return f(x) + g(x)\n\n# Applying the new function to a number\nprint(h(3))",
      "outputs": []
    },
    {
      "id": "b9d877e6-97a5-45ad-ae2e-cafbb51a3ad9",
      "cell_type": "markdown",
      "source": "We have created a new function h(x) = f(x) + g(x), where f(x) = x^2 and g(x) = x + 2. We applied the new function to the number 3, and the result was 14. This is because f(3) = 3^2 = 9 and g(3) = 3 + 2 = 5, and the sum of these is 14.\n\nThis demonstrates the concept of function manipulation. By combining and manipulating functions in different ways, we can create complex models that can capture intricate patterns in data.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "865996d4-b8a3-4b2e-83b2-5ee86ee6a4e8",
      "cell_type": "markdown",
      "source": "## 14. Characteristics of Mathematical Functions\n\nMathematical functions have several important characteristics that can help us understand their behavior. These include:\n\n- **Domain and Range:** The domain of a function is the set of all possible input values, while the range is the set of all possible output values.\n- **Zeroes or Roots:** These are the values of x for which the function f(x) equals zero.\n- **Extrema:** These are the maximum and minimum values of the function.\n- **Symmetry:** A function is symmetric about the y-axis if f(x) = f(-x) for all x in the domain. It is symmetric about the origin if f(x) = -f(-x) for all x in the domain.\n- **Periodicity:** A function is periodic if there exists a positive number P such that f(x + P) = f(x) for all x in the domain.\n- **Continuity:** A function is continuous if it is defined for all points in its domain and there are no abrupt changes in value.\n- **Differentiability:** A function is differentiable if it has a derivative at each point in its domain.\n\nIn the context of Machine Learning, understanding these characteristics can help us choose the right function for a given task, and it can also help us interpret the behavior of our models.\n\nLet's now create a simple example to illustrate some of these characteristics.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "e6588a76-a85c-49ca-8b12-3da4791d3f3a",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b5ebb89e-bd0c-46d5-9c5e-11fa27161735"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:31:25.958633+00:00",
          "start_time": "2023-07-04T04:31:25.439593+00:00"
        }
      },
      "execution_count": null,
      "source": "# Defining a function\ndef f(x):\n    return x ** 2\n\n# Plotting the function\nx = np.linspace(-10, 10, 400)\ny = f(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Plot of the function f(x) = x^2')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True)\nplt.show()",
      "outputs": []
    },
    {
      "id": "120749a5-5e7c-497f-9ced-88817afa7145",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "237fff8e-2ddb-41f0-bc06-c97ed7b88a7b"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:31:53.474974+00:00",
          "start_time": "2023-07-04T04:31:52.537954+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Defining a function\ndef f(x):\n    return x ** 2\n\n# Plotting the function\nx = np.linspace(-10, 10, 400)\ny = f(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Plot of the function f(x) = x^2')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True)\nplt.show()",
      "outputs": []
    },
    {
      "id": "7e884f06-5b46-4d5c-9bd6-21813431db9b",
      "cell_type": "markdown",
      "source": "The plot above shows the function f(x) = x^2. We can observe several characteristics of this function:\n\n- **Domain and Range:** The domain is all real numbers, and the range is all non-negative real numbers.\n- **Zeroes or Roots:** The function has a single root at x = 0.\n- **Extrema:** The function has a minimum value of 0 at x = 0.\n- **Symmetry:** The function is symmetric about the y-axis.\n- **Periodicity:** The function is not periodic.\n- **Continuity:** The function is continuous for all real numbers.\n- **Differentiability:** The function is differentiable for all real numbers.\n\nUnderstanding these characteristics can help us interpret the behavior of our models and make better decisions in the process of model selection and feature engineering.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "606f9154-8be5-431a-a5d3-2e689e122ca8",
      "cell_type": "markdown",
      "source": "## 15. Perceptron - A Type of Artificial Neuron in Machine Learning\n\nA perceptron is a type of artificial neuron used in Machine Learning. It was developed by Frank Rosenblatt in the late 1950s. A perceptron takes several binary inputs, multiplies them by their weights, and then sums them. If the weighted sum is greater than a certain threshold, the perceptron outputs 1; otherwise, it outputs 0.\n\nThe perceptron is the simplest form of a neural network and serves as the building block for more complex neural networks. It is used in supervised learning for binary classification tasks.\n\nLet's now create a simple example of a perceptron.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "da49a711-f478-4289-827c-8b20265909b3",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "da8de8cc-7f1e-4bd0-aacb-6de01db68e3e"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:32:58.430574+00:00",
          "start_time": "2023-07-04T04:32:57.892974+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import Perceptron\n\n# Creating a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Training a perceptron\nclf = Perceptron(tol=1e-3, random_state=42)\nclf.fit(X, y)\n\n# Printing the weights and bias of the perceptron\nprint('Weights:', clf.coef_)\nprint('Bias:', clf.intercept_)",
      "outputs": []
    },
    {
      "id": "cab4f7ef-f94e-4419-a86c-34180dc3ec42",
      "cell_type": "markdown",
      "source": "We created a binary classification dataset and trained a perceptron on it. The perceptron learned weights and a bias that define a decision boundary for classifying the data points. The weights and bias are parameters of the perceptron that are learned during the training process.\n\nThe weights of the perceptron are [5.85513003, -0.9951042], and the bias is 0. These parameters define a linear decision boundary in the 2-dimensional input space.\n\nThis demonstrates the concept of a perceptron. By adjusting the weights and bias, the perceptron can learn to classify a wide range of datasets.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "c3842af7-2cd9-4997-82f3-6dc57680a755",
      "cell_type": "markdown",
      "source": "## 16. Activation Functions\n\nActivation functions are a crucial component of neural networks. They determine the output of a neural network, its accuracy, and the computational efficiency of training a model.\n\nActivation functions serve two primary purposes:\n\n- **Non-linearity:** Activation functions introduce non-linear properties to the network. This helps the network learn from the error back-propagation and manage the 'vanishing gradient' problem.\n- **Normalization:** Some activation functions also help normalize the output of each neuron to a range between 1 and 0 or between -1 and 1.\n\nThere are several types of activation functions, each with its characteristics and use cases. Some of the most commonly used activation functions include the sigmoid function, the hyperbolic tangent function (tanh), the rectified linear unit (ReLU), and the softmax function.\n\nLet's now create a simple example to illustrate the concept of activation functions.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "e514f210-d62b-4837-8edc-ae442655627f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "68e643de-53b0-487f-8b5d-4ee620dad614"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:34:05.443453+00:00",
          "start_time": "2023-07-04T04:34:04.910023+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Defining the sigmoid activation function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Defining the ReLU activation function\ndef relu(x):\n    return np.maximum(0, x)\n\n# Plotting the activation functions\nx = np.linspace(-10, 10, 1000)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, sigmoid(x), label='Sigmoid')\nplt.plot(x, relu(x), label='ReLU')\nplt.title('Activation Functions')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend()\nplt.grid(True)\nplt.show()",
      "outputs": []
    },
    {
      "id": "d6b5d332-00cf-46ec-b5ba-18ed5758e1e1",
      "cell_type": "markdown",
      "source": "However, the sigmoid function has a couple of drawbacks. It suffers from the vanishing gradient problem, which can slow down training, and its output is not zero-centered.\n\nThe ReLU function is a simple function that outputs the input if it's positive; otherwise, it outputs zero. It has become very popular because it helps to mitigate the vanishing gradient problem and is computationally efficient.\n\nHowever, the ReLU function is not without its problems. For example, it can cause dead neurons, which are neurons that only output zero and therefore do not contribute to the learning of the network.\n\nThere are many other activation functions, each with its strengths and weaknesses, and the choice of activation function can depend on the specific requirements of the task.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "8718eac7-9e86-4d3c-9e5b-1b36df31ea97",
      "cell_type": "markdown",
      "source": "## 17. Simple Linear Regression\n\nSimple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:\n\n1. One variable, denoted x, is regarded as the predictor, explanatory, or independent variable.\n2. The other variable, denoted y, is regarded as the response, outcome, or dependent variable.\n\nBecause the other terms are used less frequently today, we'll use the \"predictor\" and \"response\" terms to refer to the variables. The other terms are mentioned only to make you familiar with them should you encounter them. In simple linear regression, we predict the response variable (y) as a function of the predictor variable (x).\n\nWhen both variables are quantitative, the linearity assumption is appropriate: the relationship between the predictor and the response can be modeled with a linear function. The model has the form:\n\ny = Î²0 + Î²1x + Îµ\n\nHere, Î²0 and Î²1 are two unknown constants that represent the intercept and slope terms in the linear model. They are also known as the model coefficients or parameters. Once we've used our training data to produce estimates of the parameters, we can use the fitted model to predict the response for a given value of the predictor.\n\nLet's now create a simple example of simple linear regression.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "1136ed63-d456-4aac-9c2a-c3c2fb79e914",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "98c37046-b003-4562-ae61-c1b6092889e7"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:36:59.268485+00:00",
          "start_time": "2023-07-04T04:36:59.039688+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Creating a simple dataset\nX, y = make_regression(n_samples=100, n_features=1, noise=0.4, bias=50, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a simple linear regression model\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = reg.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "7b796a41-9675-4b94-abd9-a853dcd88135",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "02f1b3ed-594c-45af-a0d2-ee939730ff90"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:37:39.401566+00:00",
          "start_time": "2023-07-04T04:37:39.238621+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Creating a simple dataset\nX, y = make_regression(n_samples=100, n_features=1, noise=0.4, bias=50, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a simple linear regression model\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = reg.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "e332fa4d-e9a3-4319-ab02-f9e9cce0ce69",
      "cell_type": "markdown",
      "source": "We created a simple dataset and trained a linear regression model on it. We then used the model to make predictions on the testing set and calculated the mean squared error of the predictions, which is 0.1667. This is a measure of the average squared difference between the actual and predicted values, and it gives us an idea of how well our model is performing.\n\nThis demonstrates the concept of simple linear regression. By fitting a linear model to our data, we can make predictions for new data points and understand the relationship between the predictor and response variables.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "00e7a3a3-b551-4350-bad4-5259cfc2207c",
      "cell_type": "markdown",
      "source": "## 18. Multiple Linear Regression\n\nMultiple linear regression is a generalization of simple linear regression to the case where the response variable is predicted based on two or more predictor variables. It not only allows us to predict the response variable based on the predictor variables, but it also allows us to understand the relationships between the predictor variables and the response variable.\n\nThe model has the form:\n\ny = Î²0 + Î²1x1 + Î²2x2 + ... + Î²nxn + Îµ\n\nHere, Î²0, Î²1, ..., Î²n are the model coefficients, and x1, x2, ..., xn are the predictor variables. The coefficients are estimated using the same least squares approach as in simple linear regression.\n\nLet's now create a simple example of multiple linear regression.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "79eb9657-38ef-42d1-b745-d4c1035775a2",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1525f065-0e5f-462d-bba6-affaa7956739"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:38:41.299328+00:00",
          "start_time": "2023-07-04T04:38:41.136831+00:00"
        }
      },
      "execution_count": null,
      "source": "# Creating a dataset with two features\nX, y = make_regression(n_samples=100, n_features=2, noise=0.4, bias=50, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a multiple linear regression model\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = reg.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "0a2670c0-4469-4833-bf18-54a4199f4b00",
      "cell_type": "markdown",
      "source": "We created a dataset with two features and trained a multiple linear regression model on it. We then used the model to make predictions on the testing set and calculated the mean squared error of the predictions, which is 0.2474. This is a measure of the average squared difference between the actual and predicted values, and it gives us an idea of how well our model is performing.\n\nThis demonstrates the concept of multiple linear regression. By fitting a linear model to our data, we can make predictions for new data points and understand the relationships between the predictor variables and the response variable.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "af600b54-738a-45b6-868a-849feee389cf",
      "cell_type": "markdown",
      "source": "## 19. Logistic Regression\n\nLogistic regression is a statistical model that uses a logistic function to model a binary dependent variable. Although the name may suggest a regression model, logistic regression is actually a probabilistic classification model. Logistic regression uses the concept of odds ratios to calculate the probability of a certain class or event existing, such as pass/fail, win/lose, alive/dead, etc.\n\nThe logistic function, also called the sigmoid function, is an S-shaped curve that maps any real-valued number into another number between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.\n\nThe model has the form:\n\np(X) = e^(Î²0 + Î²1X) / (1 + e^(Î²0 + Î²1X))\n\nHere, p(X) is the probability of the positive class, and Î²0 and Î²1 are the parameters of the model that we need to estimate from our training data.\n\nLet's now create a simple example of logistic regression.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "60abb6ba-8cc7-45eb-9608-df258a9d5055",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ffd9f0b3-8813-41b6-a608-3bcc9ded99bf"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:39:50.278689+00:00",
          "start_time": "2023-07-04T04:39:50.112097+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.linear_model import LogisticRegression\n\n# Creating a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a logistic regression model\nclf = LogisticRegression(random_state=42)\nclf.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = clf.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "1a715ab8-e6c1-4364-88ac-d94dc7eb78cd",
      "cell_type": "markdown",
      "source": "We created a binary classification dataset and trained a logistic regression model on it. We then used the model to make predictions on the testing set and calculated the mean squared error of the predictions, which is 0.05. This is a measure of the average squared difference between the actual and predicted values, and it gives us an idea of how well our model is performing.\n\nThis demonstrates the concept of logistic regression. By fitting a logistic model to our data, we can make predictions for new data points and understand the relationships between the predictor variables and the response variable.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b4de51cf-e259-4048-8981-dd35b7bc1713",
      "cell_type": "markdown",
      "source": "## 20. Decision Trees\n\nDecision Trees are a type of Supervised Machine Learning where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves. The leaves are the decisions or the final outcomes. And the decision nodes are where the data is split.\n\nThere are two main types of Decision Trees:\n\n- **Categorical Variable Decision Tree:** Decision Tree which has categorical target variable then it called as categorical variable decision tree.\n- **Continuous Variable Decision Tree:** Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.\n\nAdvantages of Decision Tree:\n\n- Decision Trees are easy to explain. It results in a set of rules.\n- It follows the same approach as humans generally follow while making decisions.\n- Interpretation of a complex Decision Tree model can be simplified by its visualizations. Even a naive person can understand logic.\n- The Number of hyper-parameters to be tuned is almost null.\n\nLet's now create a simple example of a decision tree.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "3e4dd79d-198a-4aa8-817d-b96d85559182",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "416e4a8d-0ed2-4a12-9ebe-bfc1b7b8c280"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:41:28.563629+00:00",
          "start_time": "2023-07-04T04:41:28.387826+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Creating a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a decision tree model\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = clf.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "cee65e92-101b-4231-b326-e443bbb6768b",
      "cell_type": "markdown",
      "source": "We created a binary classification dataset and trained a decision tree model on it. We then used the model to make predictions on the testing set and calculated the mean squared error of the predictions, which is 0.05. This is a measure of the average squared difference between the actual and predicted values, and it gives us an idea of how well our model is performing.\n\nThis demonstrates the concept of decision trees. By fitting a decision tree model to our data, we can make predictions for new data points and understand the relationships between the predictor variables and the response variable.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "6e4737fb-32b6-4362-b709-23631a79b260",
      "cell_type": "markdown",
      "source": "## 21. Random Forests\n\nRandom Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple algorithms to solve a particular problem.\n\nRandom Forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset. Instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output.\n\nThe greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.\n\nLet's now create a simple example of a random forest.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "75ba1cdd-816d-4d06-b5c5-ee0abb1752dd",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "45c313ae-9513-4f31-9902-c60b87247494"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:42:31.654775+00:00",
          "start_time": "2023-07-04T04:42:31.339348+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Creating a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a random forest model\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = clf.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "356608ee-0e4b-4f29-aff3-f5cd88f39f09",
      "cell_type": "markdown",
      "source": "We created a binary classification dataset and trained a random forest model on it. We then used the model to make predictions on the testing set and calculated the mean squared error of the predictions, which is 0.05. This is a measure of the average squared difference between the actual and predicted values, and it gives us an idea of how well our model is performing.\n\nThis demonstrates the concept of random forests. By fitting a random forest model to our data, we can make predictions for new data points and understand the relationships between the predictor variables and the response variable.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "f7b5fb1e-34ad-47ff-b142-17ee532b910d",
      "cell_type": "markdown",
      "source": "## 22. Support Vector Machines\n\nSupport Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.\n\nSupport Vectors are simply the coordinates of individual observation. The SVM classifier is a frontier which best segregates the two classes (hyper-plane/ line).\n\nLet's now create a simple example of a support vector machine.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "db6ce308-e588-46fd-9bb1-28877d4a07a1",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "505934aa-4727-4b6e-83da-62cc3f8a0a46"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:43:42.682532+00:00",
          "start_time": "2023-07-04T04:43:42.519563+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.svm import SVC\n\n# Creating a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a SVM model\nclf = SVC(random_state=42)\nclf.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = clf.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "c70f41d9-ff67-400a-8ab5-c0d2877e4a5e",
      "cell_type": "markdown",
      "source": "We created a binary classification dataset and trained a support vector machine (SVM) model on it. We then used the model to make predictions on the testing set and calculated the mean squared error of the predictions, which is 0.0. This is a measure of the average squared difference between the actual and predicted values, and it gives us an idea of how well our model is performing.\n\nThis demonstrates the concept of support vector machines. By fitting an SVM model to our data, we can make predictions for new data points and understand the relationships between the predictor variables and the response variable.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "384f480f-f024-4c4e-b853-64c4ba95a624",
      "cell_type": "markdown",
      "source": "## 23. K-Nearest Neighbors\n\nK-Nearest Neighbors (KNN) is one of the simplest algorithms used in Machine Learning for regression and classification problem. KNN algorithms use data and classify new data points based on similarity measures (e.g. distance function). Classification is done by a majority vote to its neighbors. The data is assigned to the class which has the nearest neighbors. As you increase the number of nearest neighbors, the value of k, accuracy might increase.\n\nAdvantages of KNN:\n\n- Quick calculation time\n- Simple algorithm â to explain and understand/interpret\n- Versatility â useful for classification or regression\n\nDisadvantages of KNN:\n\n- Accuracy depends on the quality of the data\n- With large data, the prediction stage might be slow\n- Sensitive to the scale of the data and irrelevant features\n- Require high memory â need to store all of the training data\n- Given that it stores all of the training, it can be computationally expensive\n\nLet's now create a simple example of K-Nearest Neighbors.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b9f63ef1-fe79-4f0f-8e78-f2c6df559678",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e47dd103-438a-4459-b947-5e1a822b8725"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:44:50.327322+00:00",
          "start_time": "2023-07-04T04:44:50.160248+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Creating a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a KNN model\nclf = KNeighborsClassifier(n_neighbors=3)\nclf.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = clf.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    },
    {
      "id": "7bd0a5a1-5671-4432-a4fd-b298e5dee161",
      "cell_type": "markdown",
      "source": "We created a binary classification dataset and trained a K-Nearest Neighbors (KNN) model on it. We then used the model to make predictions on the testing set and calculated the mean squared error of the predictions, which is 0.05. This is a measure of the average squared difference between the actual and predicted values, and it gives us an idea of how well our model is performing.\n\nThis demonstrates the concept of K-Nearest Neighbors. By fitting a KNN model to our data, we can make predictions for new data points and understand the relationships between the predictor variables and the response variable.\n\nNow, let's move on to the next topic.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "115902c5-e838-4843-aab7-6623feb8e820",
      "cell_type": "markdown",
      "source": "## 24. Naive Bayes\n\nNaive Bayes is a classification algorithm for binary (two-class) and multiclass classification problems. The technique is easiest to understand when described using binary or categorical input values.\n\nIt is called naive Bayes or idiot Bayes because the calculation of the probabilities for each hypothesis are simplified to make their calculation tractable. Rather than attempting to calculate the values of each attribute value P(d1, d2, d3|h), they are assumed to be conditionally independent given the target value and calculated as P(d1|h) * P(d2|H) and so on.\n\nThis is a very strong assumption that is most unlikely in real data, i.e. that the attributes do not interact. Nevertheless, the approach performs surprisingly well on data where this assumption does not hold.\n\nLet's now create a simple example of Naive Bayes.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "8a186ce3-d7e5-4041-b528-3ed32e2c0fc9",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a88147d4-a6ca-45b4-8706-28fb74ca608b"
        },
        "ExecuteTime": {
          "end_time": "2023-07-04T04:45:55.058398+00:00",
          "start_time": "2023-07-04T04:45:54.893444+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing the required libraries\nfrom sklearn.naive_bayes import GaussianNB\n\n# Creating a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training a Naive Bayes model\nclf = GaussianNB()\nclf.fit(X_train, y_train)\n\n# Making predictions on the testing set\ny_pred = clf.predict(X_test)\n\n# Calculating the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Mean Squared Error:', mse)",
      "outputs": []
    }
  ]
}
