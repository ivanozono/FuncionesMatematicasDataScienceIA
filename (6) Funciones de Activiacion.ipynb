{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "8e1130d9-6883-5e66-bd37-5f80261cd4de",
        "openai_ephemeral_user_id": "f4100e82-ce6d-50e9-8938-a98361474c54",
        "openai_subdivision1_iso_code": "MX-BCN"
      }
    },
    "noteable": {
      "last_transaction_id": "51926151-4aff-4e41-96a1-d80d3ad42ce5"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "b658a80a-4639-4a7d-848d-4e7f523abe56",
      "cell_type": "markdown",
      "source": "# Activation Functions in Neural Networks\n\nIn this notebook, we will explore the concept of activation functions in neural networks. Activation functions are a crucial component of neural networks as they determine the output of a neuron based on its input.\n\nWe will cover the following topics:\n\n1. Introduction to activation functions\n2. Common types of activation functions\n3. Implementation of activation functions in Python\n4. Use cases of different activation functions\n\nLet's get started!",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "7305541b-cc40-4a73-82c4-aef783c0cf40",
      "cell_type": "markdown",
      "source": "## 1. Introduction to Activation Functions\n\nIn artificial neural networks, the activation function of a neuron defines the output of that neuron given a set of inputs. Technically, the activation function defines the mapping of a neuron's weighted input to its output. This function is also often referred to as the transfer function.\n\nThe purpose of the activation function is to introduce non-linearity into the output of a neuron. This is important because most real world data is non linear and we want neurons to learn these non linear representations.\n\nEvery activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice:",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "06ba1071-adb6-429e-9d3a-5de5f2c5664b",
      "cell_type": "markdown",
      "source": "## 2. Common Types of Activation Functions\n\nThere are many types of activation functions, each with its own advantages and disadvantages. Here are a few common ones:\n\n- **Sigmoid Function**: The sigmoid function is a type of activation function that is traditionally very popular with neural networks. The input to the function is transformed into a value between 0.0 and 1.0. Inputs that are much larger than 1.0 are transformed to the value 1.0, similarly, values much smaller than 0.0 are snapped to 0.0.\n\n- **ReLU (Rectified Linear Unit) Function**: The ReLU function is currently the most popular activation function for deep neural networks. The function returns 0 if the input is negative, and the input if the input is positive or 0.\n\n- **Tanh (Hyperbolic Tangent) Function**: The tanh function is a rescaled version of the sigmoid function. The input to the function is transformed into a value between -1.0 and 1.0.\n\nLet's implement these functions in Python.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "e3d20724-2f02-47c9-9a53-7a95e65275ab",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "037f4a8d-ac73-4e04-9d1d-6d99e736f84c"
        },
        "ExecuteTime": {
          "end_time": "2023-07-03T19:40:27.319689+00:00",
          "start_time": "2023-07-03T19:40:25.862186+00:00"
        }
      },
      "execution_count": null,
      "source": "# Importing Required Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sigmoid Function\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function\"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# ReLU Function\ndef relu(x):\n    \"\"\"ReLU activation function\"\"\"\n    return np.maximum(0, x)\n\n# Tanh Function\ndef tanh(x):\n    \"\"\"Tanh activation function\"\"\"\n    return np.tanh(x)\n\n# Inputs\nx = np.linspace(-10, 10, 100)\n\n# Compute activation function values\ny_sigmoid = sigmoid(x)\ny_relu = relu(x)\ny_tanh = tanh(x)\n\n# Create subplots\nfig, ax = plt.subplots(3, 1, figsize=(5, 15))\n\n# Plot Sigmoid\nax[0].plot(x, y_sigmoid)\nax[0].set_title('Sigmoid Function')\n\n# Plot ReLU\nax[1].plot(x, y_relu)\nax[1].set_title('ReLU Function')\n\n# Plot Tanh\nax[2].plot(x, y_tanh)\nax[2].set_title('Tanh Function')\n\n# Show the plot\nplt.tight_layout()\nplt.show()",
      "outputs": []
    },
    {
      "id": "e5943d65-a5ad-4c5d-9168-b7bd7bff6cae",
      "cell_type": "markdown",
      "source": "## 3. Use Cases of Different Activation Functions\n\nEach activation function has its own advantages and is suitable for different kinds of tasks. Here are some general use cases:\n\n- **Sigmoid functions** and their combinations are usually used in feed-forward neural networks.\n\n- **Tanh functions** are also sigmoidal (s-shaped) and are mainly used classification between two classes.\n\n- **ReLU functions** are primarily used in hidden layers of Neural network. ReLU is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. One of the limitations of ReLU is that it can only be used within Hidden layers of a Neural Network Model.\n\nIt's important to note that one type of activation function is not necessarily better than the others. The best activation function depends on the specific application and on the specific layer in the neural network. In practice, ReLU is often used for the hidden layers, and a sigmoid or softmax function is used for the output layer.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    }
  ]
}